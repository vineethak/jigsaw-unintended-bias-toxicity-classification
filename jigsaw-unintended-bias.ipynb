{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T18:15:07.675702Z",
     "start_time": "2019-08-01T18:15:06.008893Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nilesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#loading required libraries\n",
    "#basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "#misc\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import seaborn as sns\n",
    "\n",
    "#nlp\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "#FeatureEngineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer=TweetTokenizer()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T18:15:07.851081Z",
     "start_time": "2019-08-01T18:15:07.677028Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T18:15:09.258837Z",
     "start_time": "2019-08-01T18:15:07.852263Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.optimizers import Adam\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T18:15:09.261424Z",
     "start_time": "2019-08-01T18:15:09.259899Z"
    }
   },
   "outputs": [],
   "source": [
    "EMB_PATH = \"crawl-300d-2M.gensim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T18:15:09.421124Z",
     "start_time": "2019-08-01T18:15:09.262226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawl-300d-2M.gensim\r\n",
      "crawl-300d-2M.gensim.vectors.npy\r\n",
      "crawl-300d-2M.gensim.vectors.npy.zip\r\n",
      "crawl-300d-2M.gensim.zip\r\n",
      "data\r\n",
      "jigsaw-unintended-bias-in-toxicity-classification.zip\r\n",
      "jigsaw-unintended-bias.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T18:15:09.435915Z",
     "start_time": "2019-08-01T18:15:09.430325Z"
    }
   },
   "outputs": [],
   "source": [
    "#!unzip jigsaw-unintended-bias-in-toxicity-classification.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T18:15:09.450759Z",
     "start_time": "2019-08-01T18:15:09.441630Z"
    }
   },
   "outputs": [],
   "source": [
    "#!unzip crawl-300d-2M.gensim.vectors.npy.zip\n",
    "#!unzip crawl-300d-2M.gensim.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T18:15:16.349291Z",
     "start_time": "2019-08-01T18:15:09.454492Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test  = pd.read_csv(\"data/test.csv\")\n",
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T18:15:16.352843Z",
     "start_time": "2019-08-01T18:15:16.350402Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_MODELS = 1\n",
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 60    #128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220\n",
    "MAX_FEATURES = 30000\n",
    "EMBED_SIZE = 300\n",
    "IDENTITY_COLUMNS = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T18:15:16.373635Z",
     "start_time": "2019-08-01T18:15:16.353748Z"
    }
   },
   "outputs": [],
   "source": [
    "AUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "TARGET_COLUMN = 'target'\n",
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-01T18:15:16.663601Z",
     "start_time": "2019-08-01T18:15:16.380621Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = train[TEXT_COLUMN].astype(str)\n",
    "y_train = train[TARGET_COLUMN].values\n",
    "y_aux_train = train[AUX_COLUMNS].values\n",
    "x_test = test[TEXT_COLUMN].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T18:16:08.940Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE, lower=False)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test)) #generates word ocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T18:16:08.942Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index #return dictionary of words and their indices in our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T18:16:08.943Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = tokenizer.texts_to_sequences(list(x_train)) #convert string to list of words\n",
    "x_test = tokenizer.texts_to_sequences(list(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T18:16:08.946Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pad sequences\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T18:16:08.947Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_embeddings(embed_dir=EMB_PATH):\n",
    "    embedding_index = KeyedVectors.load(embed_dir, mmap= 'r')\n",
    "    return embedding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T18:16:08.950Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_index = load_embeddings(EMB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T18:16:08.952Z"
    }
   },
   "outputs": [],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T18:16:08.954Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_matrix(word_index=word_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1,300))\n",
    "    for word , i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_matrix[i] = embeddings_index[\"unknown\"]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T18:16:08.957Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix = build_matrix(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T18:16:08.958Z"
    }
   },
   "outputs": [],
   "source": [
    "del embeddings_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T18:16:08.960Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix=embedding_matrix, verbose=0):\n",
    "    input_sequences = Input(shape=(MAX_LEN,), dtype='int32') #placeholder for input in this case text sequence\n",
    "    x = Embedding(len(word_index) + 1, EMBED_SIZE , weights=[embedding_matrix], trainable=False)(input_sequences)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    #x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    \n",
    "    avg_pool1 = GlobalAveragePooling1D()(x)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = concatenate([avg_pool1, max_pool1])\n",
    "    dense = Dense(DENSE_HIDDEN_UNITS, activation='relu')(x)\n",
    "    \n",
    "    x = add([x, dense])\n",
    "    #x = add([x, dense])\n",
    "    \n",
    "    preds = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(input_sequences, preds)\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    if compile:\n",
    "        model.compile(loss='binary_crossentropy',optimizer=Adam(0.005),metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-01T18:16:08.962Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "splits = list(KFold(n_splits=2).split(x_train,y_train))\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "oof_preds = np.zeros((x_train.shape[0]))\n",
    "test_preds = np.zeros((x_test.shape[0]))\n",
    "for fold in [0,1,2,3,4]:\n",
    "    K.clear_session()\n",
    "    tr_ind, val_ind = splits[fold]\n",
    "    print(tr_ind, val_ind)\n",
    "    ckpt = ModelCheckpoint(f'gru_{fold}.hdf5', save_best_only = True)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "    model = build_model()\n",
    "    model.fit(x_train[tr_ind],\n",
    "        y_train[tr_ind]>0.5,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        validation_data=(x_train[val_ind], y_train[val_ind]>0.5),\n",
    "        callbacks = [es,ckpt])\n",
    "\n",
    "    oof_preds[val_ind] += model.predict(x_train[val_ind])[:,0]\n",
    "    test_preds += model.predict(x_test)[:,0]\n",
    "test_preds /= 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
